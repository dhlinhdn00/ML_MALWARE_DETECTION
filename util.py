import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import KFold

def load_data(filepath, sep='|'):
    """ Load dataset from a file """
    return pd.read_csv(filepath, sep=sep)

def prepare_data(dataset, drop_columns, test_size=0.20, random_state=0):
    """ Prepare data for training and testing """
    X = dataset.drop(drop_columns, axis=1).values
    y = dataset['legitimate'].values
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)
    return X_train, X_test, y_train, y_test

def scale_data(X_train, X_test):
    """ Scale data using StandardScaler """
    sc = StandardScaler()
    X_train_scaled = sc.fit_transform(X_train)
    X_test_scaled = sc.transform(X_test)
    return X_train_scaled, X_test_scaled

def display_model_performance(y_test, y_pred, cv_mean=None, cv_std=None, name='Model'):
    """ Display confusion matrix, classification report, and cross-validation details """
    print("-----Model: " + name + " -----")
    cm = confusion_matrix(y_test, y_pred)
    print("Confusion Matrix:")
    print(cm)
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))
    print("Overall Accuracy: {:.2f}%".format(accuracy_score(y_test, y_pred) * 100))

    if cv_mean is not None and cv_std is not None:
        print("Cross-validation Mean Accuracy: {:.2f}%".format(cv_mean * 100))
        print("Standard Deviation: {:.2f}%".format(cv_std * 100))

    plt.figure(figsize=(10, 7))
    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues')
    plt.xlabel('Predicted labels')
    plt.ylabel('True labels')
    plt.title('Confusion Matrix')
    plt.show()

def perform_knn_cv(X_train, y_train, max_k=50, folds=20):
    """ Perform cross-validation to find the optimal number of neighbors """
    cv_scores = []
    for k in range(1, max_k, 2):  
        knn = KNeighborsClassifier(n_neighbors=k)
        scores = cross_val_score(knn, X_train, y_train, cv=folds, scoring='accuracy')
        cv_scores.append(scores.mean())

    MSE = [1 - x for x in cv_scores]
    optimal_k = (2 * np.argmin(MSE) + 1)
    return optimal_k, cv_scores

def evaluate_model(create_model, X, y, epochs=50, batch_size=32, n_splits=10):
    scores = []
    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)
    for train, test in kfold.split(X, y):
        model = create_model(X.shape[1])
        model.fit(X[train], y[train], epochs=epochs, batch_size=batch_size, verbose=0)
        _, accuracy = model.evaluate(X[test], y[test], verbose=0)
        scores.append(accuracy)
    return scores