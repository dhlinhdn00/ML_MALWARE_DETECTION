import numpy as np
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

class RandomForest:
    def __init__(self, n_estimators=50, max_depth=None, min_samples_split=2):
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.trees = []

    def fit(self, X, y):
        self.trees = []
        n_samples, n_features = X.shape
        for _ in range(self.n_estimators):
            indices = np.random.choice(n_samples, n_samples, replace=True)
            tree = self._build_tree(X[indices], y[indices], depth=0)
            self.trees.append(tree)

    def predict(self, X):
        tree_preds = np.array([self._predict_tree(tree, X) for tree in self.trees])
        return np.round(tree_preds.mean(axis=0)).astype(int)

    def _build_tree(self, X, y, depth):
        n_samples, n_features = X.shape
        if n_samples < self.min_samples_split or len(np.unique(y)) == 1 or (self.max_depth is not None and depth >= self.max_depth):
            return {'value': np.round(y.mean())}
        feature_idx = np.random.randint(0, n_features)
        threshold = np.median(X[:, feature_idx])
        left_idx = X[:, feature_idx] <= threshold
        right_idx = X[:, feature_idx] > threshold

        if left_idx.sum() == 0 or right_idx.sum() == 0:
            return {'value': np.round(y.mean())}

        return {
            'feature_idx': feature_idx,
            'threshold': threshold,
            'left': self._build_tree(X[left_idx], y[left_idx], depth + 1),
            'right': self._build_tree(X[right_idx], y[right_idx], depth + 1)
        }

    def _predict_tree(self, tree, X):
        if 'value' in tree:
            return np.ones(X.shape[0]) * tree['value']
        left_idx = X[:, tree['feature_idx']] <= tree['threshold']
        right_idx = X[:, tree['feature_idx']] > tree['threshold']
        predictions = np.empty(X.shape[0])
        predictions[left_idx] = self._predict_tree(tree['left'], X[left_idx])
        predictions[right_idx] = self._predict_tree(tree['right'], X[right_idx])
        return predictions

    def score(self, X, y):
        y_pred = self.predict(X)
        return np.mean(y_pred == y)

    def get_params(self, deep=True):
        return {"n_estimators": self.n_estimators, "max_depth": self.max_depth, "min_samples_split": self.min_samples_split}

    def set_params(self, **params):
        for key, value in params.items():
            setattr(self, key, value)
        return self

class KNN:
    def __init__(self, n_neighbors=7):
        self.n_neighbors = n_neighbors
        self.X_train = None
        self.y_train = None

    def fit(self, X, y):
        self.X_train = X
        self.y_train = y

    def predict(self, X):
        y_pred = np.empty(X.shape[0])
        for i, x in enumerate(X):
            distances = np.sqrt(((self.X_train - x) ** 2).sum(axis=1))
            neighbor_indices = np.argsort(distances)[:self.n_neighbors]
            neighbor_votes = self.y_train[neighbor_indices]
            y_pred[i] = np.bincount(neighbor_votes).argmax()
        return y_pred.astype(int)

    def score(self, X, y):
        y_pred = self.predict(X)
        return np.mean(y_pred == y)

    def get_params(self, deep=True):
        return {"n_neighbors": self.n_neighbors}

    def set_params(self, **params):
        for key, value in params.items():
            setattr(self, key, value)
        return self

class NaiveBayes:
    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.classes = np.unique(y)
        self.class_prior = np.array([np.sum(y == c) for c in self.classes]) / n_samples
        self.class_mean = np.array([X[y == c].mean(axis=0) for c in self.classes])
        self.class_var = np.array([X[y == c].var(axis=0) for c in self.classes])

    def predict(self, X):
        y_pred = [self._predict_single(x) for x in X]
        return np.array(y_pred).astype(int)

    def _predict_single(self, x):
        posteriors = []
        for idx, c in enumerate(self.classes):
            prior = np.log(self.class_prior[idx])
            conditional = -0.5 * np.sum(np.log(2. * np.pi * self.class_var[idx]))
            conditional -= 0.5 * np.sum(((x - self.class_mean[idx]) ** 2) / self.class_var[idx])
            posterior = prior + conditional
            posteriors.append(posterior)
        return self.classes[np.argmax(posteriors)]

    def score(self, X, y):
        y_pred = self.predict(X)
        return np.mean(y_pred == y)

    def get_params(self, deep=True):
        return {}

    def set_params(self, **params):
        return self

def simple_neural_network(input_dim):
    model = Sequential([
        Dense(128, activation='relu', input_shape=(input_dim,)),
        Dropout(0.5),
        Dense(64, activation='relu'),
        Dropout(0.5),
        Dense(1, activation='sigmoid')
    ])
    
    model.compile(optimizer='adam',
                  loss='binary_crossentropy',
                  metrics=['accuracy'])
    return model

def random_forest(X_train, y_train):
    classifier = RandomForest(n_estimators=50, max_depth=None)
    classifier.fit(X_train, y_train)
    return classifier

def xgboost(X_train, y_train):
    classifier = XGBClassifier(max_depth=20, learning_rate=0.3, n_estimators=150)
    classifier.fit(X_train, y_train)
    return classifier

def knn(X_train, y_train, n_neighbors=7, metric='minkowski', p=2):
    classifier = KNN(n_neighbors=n_neighbors)
    classifier.fit(X_train, y_train)
    return classifier

def lightgbm(X_train, y_train):
    classifier = LGBMClassifier(n_estimators=100, learning_rate=0.1, random_state=0)
    classifier.fit(X_train, y_train)
    return classifier

def catboost(X_train, y_train):
    classifier = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=10, verbose=0, random_state=0)
    classifier.fit(X_train, y_train)
    return classifier

def naive_bayes(X_train, y_train):
    classifier = NaiveBayes()
    classifier.fit(X_train, y_train)
    return classifier

def simple_neural_network(input_dim):
    model = Sequential([
        Dense(128, activation='relu', input_shape=(input_dim,)),
        Dropout(0.5),
        Dense(64, activation='relu'),
        Dropout(0.5),
        Dense(1, activation='sigmoid')
    ])
    
    model.compile(optimizer='adam',
                  loss='binary_crossentropy',
                  metrics=['accuracy'])
    return model